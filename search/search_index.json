{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Add Attachment","text":""},{"location":"#welcome-to-github-page-for-the-add-attachment-project","title":"Welcome to GitHub Page for the Add Attachment Project","text":""},{"location":"#add-attachment_1","title":"Add Attachment","text":""},{"location":"#description","title":"Description","text":"<p>Add Attachment is a research project initiated by the KUL department Clinical Psychology.</p> <p>in Dutch:</p> <p>De \"Learning Theory of Attachment\" stelt dat de ontwikkeling van gehechtheid minstens gedeeltelijk ontvouwt als gevolg van leerprocessen waarbij het kind leert dat de zorgfiguur een veiligheidssignaal is dat beschermt tegen stress. Elke zorginteractie tussen een kind en een zorgfiguur kan gezien worden als een leermoment. Indien de contingentie, dit is het samen voorkomen, van de zorgfiguur en het geven van steun in stressvolle situaties hoog is, zal de mate van vertrouwen dat het kind heeft in de zorgfiguur toenemen. Een lage contingentie van de zorgfiguur en het bieden van steun, zal het vertrouwen van het kind in de zorgfiguur doen dalen. Deze Virtual Reality-studie is gebaseerd op het cyberball-paradigma waarbij kinderen via een simulatie een spel spelen met andere kinderen waar ze uitsluiting/competitie zullen ervaren. Hierbij zal de mate van contingentie van de zorgfiguur en het bieden van comfort trial per trial gemanipuleerd worden om het effect te onderzoeken op het vertrouwensleren van kinderen in een zorgfiguur.</p> <p>A VR environment will be created in which we'll capture biometrics as well as user responses.</p>"},{"location":"#collaboration","title":"Collaboration","text":"<ul> <li>KU Leuven Clinical Psychology:<ol> <li>Guy Bosmans</li> <li>Marlies Wintmolders</li> <li>Samu\u00ebl Budniok</li> </ol> </li> <li>LUCA school of Arts<ol> <li>Wim Forceville</li> <li>Wouter Devriese</li> <li>Kasper Jordaens</li> </ol> </li> </ul>"},{"location":"#repository-structure","title":"Repository Structure","text":"<p>in game design folder you can find the description of the game mechanics and actors.</p> <p>in hardware there's more information regarding the used hardware</p>"},{"location":"#additional-information","title":"Additional Information","text":"<p>check out the interesting links page</p>"},{"location":"Timeline/","title":"Timeline","text":""},{"location":"Timeline/#milestones","title":"Milestones","text":"<p>decide concept: June 2022 </p> <pre><code>1. Emotional + practical\n2. Emotional + supportive\n3. Competitive + practical\n4. Competitive + supportive\n</code></pre> <p>Integration of EEG+EDA: </p> <p>decision of what needs to be captured?</p> <p>offline storage ok?</p> <p>Modelling + animation: from July/August on</p> <p>=&gt;  Navragen bij project:     - hoe zo realistisch mogelijk spelers met bal?     - wie characters maken?     - wat kunnen we doen met bestaande karakters?     - waarop kunnen zij inspringen?</p> <p>first trial in-house?</p> <p>April 2023 finale oplevering</p>"},{"location":"Timeline/#mid-juni-2022","title":"Mid Juni 2022","text":"<p>beslissen concept?</p>"},{"location":"Timeline/#augustus-2022","title":"Augustus 2022","text":"<p>afkloppen script</p>"},{"location":"Timeline/#1-april-2023","title":"1 April 2023","text":"<p>1 concept uitgewerkt, met GSR en EDA </p>"},{"location":"Workpackages/","title":"work packages","text":"<p>Overview of the workpackages and their content</p>"},{"location":"Workpackages/#wp0-story-game-design","title":"WP0 Story &amp; Game Design","text":""},{"location":"Workpackages/#wp1-hardware-data-connection-streams","title":"WP1 Hardware: data connection &amp; streams","text":""},{"location":"Workpackages/#wp2-gamemechanic-slingshot","title":"WP2 Gamemechanic: slingshot","text":""},{"location":"Workpackages/#wp3-caregiver-graphic-design","title":"WP3 Caregiver graphic design","text":""},{"location":"Workpackages/#wp4-interaction-with-caregiver","title":"WP4 Interaction with caregiver","text":""},{"location":"Workpackages/#wp5-npc-character-design-behavior-programming","title":"WP5 NPC: character design &amp; behavior programming","text":""},{"location":"Workpackages/#wp6-score-board","title":"WP6 Score board","text":""},{"location":"Workpackages/#wp7-leveldesign-environment","title":"WP7 Leveldesign environment","text":""},{"location":"Workpackages/#wp8","title":"WP8","text":""},{"location":"Workpackages/#wp9-sound-design","title":"WP9 Sound Design","text":""},{"location":"Workpackages/#wp10-cicd-pipeline-building-the-app","title":"WP10 CI/CD Pipeline - Building the app","text":""},{"location":"Workpackages/#wp11-data-analytics","title":"WP11 Data-analytics","text":""},{"location":"Workpackages/#wp12-uxuser-journey","title":"WP12 UX/User journey","text":""},{"location":"Workpackages/#wp13-project-communication","title":"WP13 Project communication","text":""},{"location":"Workpackages/#wp-14-physical-hardware-adaptions","title":"WP 14 physical hardware adaptions","text":""},{"location":"Workpackages/#wp15-service-after-delivery","title":"WP15 Service after delivery","text":""},{"location":"install/","title":"Installation","text":""},{"location":"install/#list-of-software","title":"List of Software","text":"<ol> <li>Unity version 2021.3.0f1 LTS</li> <li>[Brainflow]https://brainflow.org/)</li> </ol>"},{"location":"install/#list-of-hardware","title":"List of Hardware","text":"<ol> <li>Pico Eye pro</li> <li>OpenBCI</li> </ol>"},{"location":"install/#installation-of-different-software-packages","title":"Installation of Different Software Packages","text":""},{"location":"install/#brainflow","title":"Brainflow","text":"<p>We want to integrate the Brainflow API with either Unity or as standalone data capture. The benefit of incorporating it in unity would be that we can easilier label and start recording data.</p> <p>To integrate it with Unity, check this link</p>"},{"location":"install/#unity","title":"Unity","text":"<p>you need:</p> <p>Pico xr SDK Tobii XR SDK unity XR interaction toolkit Nuget Package manager install WebSocketSharp-netstandard via NuGet</p>"},{"location":"links/","title":"Page for Useful Links to the Project","text":"<ul> <li>eeg notebooks</li> <li>bow in vr</li> <li>basketball in unity</li> </ul> <p>return to main page?</p> <p>bijhouden van data op device</p>"},{"location":"architecture/readme/","title":"Add Attachment","text":""},{"location":"architecture/readme/#project-intro","title":"Project Intro","text":"<p>"},{"location":"architecture/readme/#table-of-contents","title":"Table of Contents","text":"<pre><code>style: number \nmin_depth: 1 \nmax_depth: 6 \n</code></pre>"},{"location":"architecture/readme/#game-goal","title":"Game Goal","text":"<p>We want to induce competitive stress for a child and monitor his/her response towards a caregiver. \u00a0</p>"},{"location":"architecture/readme/#game-flow","title":"Game Flow","text":""},{"location":"architecture/readme/#high-level-flow","title":"High Level Flow","text":""},{"location":"architecture/readme/#a-child-enters-in-physical-space","title":"A Child Enters in Physical Space","text":"<ol> <li>The child takes place on a chair. Firstly the emotibit stress measurement device gets attached to him.</li> <li>Next we place the OpenBCI EEG headset on the head. We test (EEG#testing) if all electrodes have been placed correctly.</li> <li>Then we place the VR headset on the head of the child and the game can start. (The VR glass has been adapted to be able to hang from above the child)</li> </ol>"},{"location":"architecture/readme/#the-child-enters-the-vr-space","title":"The Child Enters the VR Space","text":"<ol> <li>The child enters the environment.</li> <li>First an Tutorial is given to the game environment and mechanics. </li> <li>A trial is run a couple of times in a row, forming a trialblock</li> <li> <p>During a trial, we measure different metrics</p> <ul> <li> <p>shooting_system: In a trial, a child is seated in which he can shoot a ball with a giant slingshot towards three pipes. </p> </li> <li> <p>targets: These pipes try to suck the ball towards them. </p> </li> <li> <p>reward_system: When we shoot the ball in the correct hole, we get some points. \u00a0</p> </li> </ul> </li> <li> <p>Alternating with us, another NPC_player shoots as well, and we see how (s)he performs. \u00a0</p> </li> <li>After 5 shots for both players, the trial is finished, and the results are shown on the score_board.\u00a0</li> <li>The caregiver gives feedback. \u00a0</li> <li>After this, you can caregiver#score the caregiver.\u00a0</li> <li>A trial#count-down timer indicates that a new trial will start, or, if we are at the end, a closing screen will be shown.</li> </ol>"},{"location":"architecture/readme/#asset-list","title":"Asset List","text":"<p>a brief overview of all assets that need to be created: assets</p>"},{"location":"architecture/readme/#timeline","title":"Timeline","text":""},{"location":"architecture/readme/#sound-design","title":"Sound Design","text":"<pre><code>#TBD\n</code></pre> <p>using FMOD</p>"},{"location":"architecture/readme/#visual-style","title":"Visual Style","text":"<p>I suggest going for a more cartoonesk Gorillaz like vibe: enough detail to get familiar with, enough abstract to be generic enough.</p> <p>Using this style, things look less realistic. Yet I believe children will understand that this is a game world. The NPCs could be their real friends.</p> <p>Also, it's easier to visualise this way results via the reward_system</p> <p></p> <p>important:</p> <p></p>"},{"location":"architecture/readme/#data-architecture","title":"Data Architecture","text":"<p>See Data streams</p>"},{"location":"architecture/Software/State%20machines/","title":"State Machines","text":"<pre><code>[*] --&gt; Start\nEndOfGame --&gt; [*]\nstate Start\n\nStart --&gt; TrialState.TrialIntro\nStart --&gt; BallState.BallInit\nStart --&gt; TrophyState.TrophyInit\nStart --&gt; CaregiverState.Idle\n\nstate TrialState {\n    state TrialIntro : start countdown \\r\\n set position of targets \\r\\n hold targets\n    state PreTrial : Reset players score \\r\\n set BallState to init \\r\\n set movement targets\n    state Trial : trialIsRunning\n    state PostTrial : Hold Targets \\r\\n if trophy given, caregiver must give feedback \\r\\n gets skipped if tutorial scene \\r\\n on exit: trialnr++\n    TrialIntro --&gt; PreTrial \n    PreTrial --&gt; Trial : startTrial signal\n\n    Trial --&gt; PostTrial : endTrial signal\n    PostTrial --&gt; PreTrial : restart\n\n    PostTrial --&gt; EndOfGame \n}\n\nstate BallState {\n    state BallInit : \"\"\n    state BallPrep : Set scoring chances\n    state BallCalcImpact : Check if we did hit the right thing\n    state BallWaiting \n    state BallLaunch \n    BallInit --&gt; BallWaiting : \"\"\n    BallWaiting --&gt; BallPrep : if trial is running\n    BallPrep --&gt; BallLaunch : ball = grabbed\n    BallLaunch --&gt; BallCalcImpact : ballDidHit\n    BallCalcImpact --&gt; BallInit : end of trial\n    BallCalcImpact --&gt; BallWaiting : not all balls have been shot\n}\n\nstate TrophyState {\n    TrophyInit --&gt; TrophyAppear : pretrial signal\n    TrophyAppear --&gt; TrophyWaiting : trophy did appear\n\n    TrophyWaiting --&gt; Giving : !trialIsRunning\n    Giving --&gt; Cooldown : once trophy given to winner\n    Cooldown --&gt; TrophyAppear : if not the end of triallist\n    Cooldown --&gt; EndOfGame : no more trials\n}\n\nstate CaregiverState {\n    Idle --&gt; Feedback : mustGiveFeedback\n    Feedback --&gt; FeedbackConfirm : if feedback spoken\n    FeedbackConfirm --&gt; Scoring : confirmed\n    Scoring --&gt; Idle : scored + confirmed\n}\nPreTrial -[#blue]-&gt; BallInit\nPreTrial -[#blue]-&gt; TrophyInit\nTrophyWaiting -[#blue]-&gt; Trial : start Trial\nGiving -[#red]-&gt; Feedback : trophy is given\nPostTrial -[#red]-&gt; Giving \nScoring -[#red]-&gt; PostTrial : restart\nBallCalcImpact -[#green]-&gt; PostTrial : End of trial\n\n\n\n</code></pre>"},{"location":"architecture/game_elements/Data%20streams/","title":"Data Streams","text":"<p>we have multiple datastreams to be connected throughout the research project.  All data eventually needs to be captured on a single computer, in order to process that later.</p> <p>Past researchers created a model stream to be used with GSR &amp; EEG data. LabStream layer, check out the documentation!</p> <p>We'll leverage this framework and use it in Unity and with our EEG and GSR devices.</p> <p>Here's an overview. </p> <pre><code>Warning, this is **not** how it's implemented in reality, this is merely to give a sense of how the datastreams. In reality all OpenBCI and emotibit data is streaming towards the PC. The LSLMarker stream injects these streams. Yet it is easier to comprehend that the LSLMarker stream intervenes directly in the hardware.\n</code></pre> <p></p>"},{"location":"architecture/game_elements/Data%20streams/#data-structure","title":"Data Structure","text":"<ul> <li>a name for the folder creation:<ul> <li>python script to create a default folder structure</li> </ul> </li> <li>per trial:<ul> <li>trial setup (contingency, what will be the response)</li> <li>score at the end?</li> <li>EOG data</li> <li>trust in caregiver</li> <li>something else? #TBD </li> </ul> </li> </ul>"},{"location":"architecture/game_elements/Data%20streams/#unity-side","title":"Unity Side","text":"<p>Unity is the Master, creating the labstream layer: LSL for Unity</p> <p>We need to decide which events we want to record. We'll give them a marker-notation:</p> Marker meaning 1 start of recording 2 kid looks at the caregiver"},{"location":"architecture/game_elements/Data%20streams/#eeg-side","title":"EEG Side","text":"<p>We'll use the Brainflow framework. This is a unified framework to be used with different EEG headsets. By creating a python script which also captures the markers, we can have the same markers as Data streams#Unity side and Data streams#GSR side.</p> <p>We'll capture the markers and inject them in the EEG data, this way we can afterwards retrieve them in the data and take a look with the oscilloscope where the interesting parts in the data are.</p>"},{"location":"architecture/game_elements/Data%20streams/#gsr-side","title":"GSR Side","text":"<p>The GSR data is recorded via an openframeworks application. We can use the LSL Marker stream to capture the data and inject the markers into the data.</p>"},{"location":"architecture/game_elements/Data%20streams/#python-websocket","title":"Python Websocket","text":"<p>We'll perhaps need a python script to open a websocket to:</p> <ul> <li>transmit name towards the Unity game?</li> <li>Get some meta-data or other data?<ul> <li>scorings of caregiver</li> <li>points taken within the game</li> <li>...</li> </ul> </li> </ul>"},{"location":"architecture/game_elements/NPC_player/","title":"NPC Player","text":"<p>The participant needs to feel competitive stress. We'll show a second participant in screen performing alongside you.  This NPC can be better or worse than the participant depending on settings.</p> <p>NPC = Non Playable Character</p> <p>There's always a player which you can see performing.</p> <p>His/her performance has NO influence whatsoever on your performance.</p> <p>He/she should always be scoring mediocre, so that we can be better than they can.</p> <p>The NPC has the same setup as we do, it's a player, sitting in a chairlike object with a huge slingshot in front of him/her.</p>"},{"location":"architecture/game_elements/NPC_player/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/NPC_player/#game-character","title":"Game Character","text":"<p>this should be a player from a list or can be just one player? #TBD </p> <p>If we want to play/compete against multiple players, we need more characters.</p>"},{"location":"architecture/game_elements/NPC_player/#interaction-with-the-slingshot","title":"Interaction with the Slingshot","text":"<p>this animation should be recorded?</p>"},{"location":"architecture/game_elements/NPC_player/#chair","title":"Chair","text":"<p>we don't get to see our own chair, yet the other chair should be clearly visible.</p>"},{"location":"architecture/game_elements/Tutorial/","title":"Tutorial","text":"<p>At the start of the first block, a child gets explanation on how the game mechanics work.</p> <p>No other player is in view. The child arrives at a white space. Only the slingshot and a ball is shown, together with the moving rings.</p> <p>No caregiver is in view</p> <p>A voice guides the child, this is a different voice than the one of the caregiver!</p> <p>We'll indicate via the Data streams that for the first block, there's a test. Yet for the second block, we can go straight to the real evaluation? #TBD </p>"},{"location":"architecture/game_elements/Tutorial/#introduction-to-game-mechanics","title":"Introduction to Game Mechanics","text":""},{"location":"architecture/game_elements/Tutorial/#stopword","title":"Stopword","text":"<p>A sign to stop the game if uncomfortable</p>"},{"location":"architecture/game_elements/Tutorial/#vr-handsets","title":"VR Handsets","text":""},{"location":"architecture/game_elements/Tutorial/#shooting-mechanics","title":"Shooting Mechanics","text":""},{"location":"architecture/game_elements/Tutorial/#colored-balls","title":"Colored Balls","text":""},{"location":"architecture/game_elements/Tutorial/#looking-at-someonesomething-has-influence","title":"Looking at someone/something Has Influence?","text":""},{"location":"architecture/game_elements/Tutorial/#text-prompts","title":"Text Prompts","text":"<p>pressing OK to move forward</p>"},{"location":"architecture/game_elements/Unity_layers/","title":"Unity Layers","text":"<ol> <li>environment</li> <li>eye</li> <li>...</li> </ol>"},{"location":"architecture/game_elements/Unity_layers/#environment","title":"Environment","text":""},{"location":"architecture/game_elements/Unity_layers/#eye","title":"Eye","text":"<p>Everything (colliders) that we want to track with the eye-tracking</p>"},{"location":"architecture/game_elements/advice/","title":"Advice","text":"<p>We get different responses based on the contingency level given to the current player. The quality (good or bad) of the advice is based on the contingency level.  We show the advice via 2 possible channels: visually &amp; auditive.</p>"},{"location":"architecture/game_elements/advice/#contingency-level","title":"Contingency Level","text":"<p>We need to know the contingency level in the game, as the response is depending on the performance of the player.</p> <p>We'll need 4 text files with possible answers:</p> <ol> <li>Good performance, cheering (good) response</li> <li>Good performance, take down (bad) response</li> <li>Bad/mediocre performance, supportive (good) response</li> <li>Bad/mediocre performance, dissappointed (bad) response</li> </ol>"},{"location":"architecture/game_elements/advice/#tbd-to-discusscontingency","title":"TBD [[to discuss#contingency]]","text":""},{"location":"architecture/game_elements/advice/#visualising-the-responses","title":"Visualising the Responses","text":"<ul> <li>2D vs 3D?<ul> <li>2D:<ul> <li>a console showing on your HUD, showing the text response with the face of your mother next to it? caregiver#HUD instead of in-game conversation</li> </ul> </li> <li>3D:<ul> <li>a text balloon appearing over the caregivers location, with some particles/bloom drawing attention</li> </ul> </li> </ul> </li> </ul>"},{"location":"architecture/game_elements/advice/#auditive-responses","title":"Auditive Responses","text":"<p>optional?</p> <p>Have someone record some responses?</p>"},{"location":"architecture/game_elements/advice/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/advice/#response-loader-asset","title":"Response Loader Asset","text":"<p>This reads the <code>string</code> (response) from the current trial and loads it.</p> <p>Once the mother is activated, we launch the visual response event.</p>"},{"location":"architecture/game_elements/advice/#visual-response-event","title":"Visual Response Event","text":"<p>the response, loaded already, gets instantiated at the playing field, either near the mother (with some particles to draw attention), or via the HUD.</p> <p>We let the response dissappear once the child has looked at the mother and x #TBD amount of seconds have passed.</p> <p>We need a confirmation button!</p>"},{"location":"architecture/game_elements/advice/#auditive-response-event","title":"Auditive Response Event","text":"<p>We have an audio player at the location of the mother. This way, it seams as if the audio is coming from her (if the child turns his/her head)</p>"},{"location":"architecture/game_elements/assets/","title":"Assets","text":"<p>a list of all assets with a graphical component</p> <ol> <li>environment<ul> <li>playground, ATM modern school gym</li> </ul> </li> <li>advice:<ul> <li>advice#Visual response event (TBD)</li> </ul> </li> <li>caregiver:<ul> <li>caregiver#rating bar: a bar to score the caregiver</li> <li>caregiver#mother</li> <li>caregiver#caregiver comms system</li> </ul> </li> <li>NPC_player:<ul> <li>One other player competing at the same game </li> <li>*NPC_player#game character<ul> <li>also NPC_player#interaction with the slingshot for this NPC</li> </ul> </li> <li>NPC_player#chair were the NPC is sitting in, this to give the impression that both children (real and NPC) have the same setup</li> </ul> </li> <li>reward_system<ul> <li>reward_system#rewards prefab</li> </ul> </li> <li>score_board:<ul> <li>score_board#score board</li> </ul> </li> <li>shooting_system:<ul> <li>shooting_system#slingshot</li> <li>shooting_system#balls</li> </ul> </li> <li>targets<ul> <li>either targets#coloured pipes</li> <li>or targets#trash cans</li> <li>or other option</li> </ul> </li> <li>trial:<ul> <li>trial#countdown timer</li> </ul> </li> <li>trialblock:<ul> <li>trialblock#welcome screen</li> </ul> </li> </ol>"},{"location":"architecture/game_elements/caregiver/","title":"Caregiver","text":""},{"location":"architecture/game_elements/caregiver/#overview","title":"Overview","text":"<p>The caregiver is the mother of the child.   She gives good or bad advice based on the advice#contingency level.   The caregiver should always be in peripheral sight of the child.</p>"},{"location":"architecture/game_elements/caregiver/#looking-at-the-caregiver","title":"Looking at the Caregiver","text":"<p>When the game is running, we note down the metrics#EOG of looking at the caregiver.</p>"},{"location":"architecture/game_elements/caregiver/#game-mechanic","title":"Game Mechanic","text":"<p>important decide how direct we should look at the caregiver. #TBD - explanation: we need to decide a boundary box where to look at. We can keep this close to her face, or her entire body, or a bit wider in case of not directly having contact? This could also give false positives on the other hand.</p> <p>Do we want some other metric instead looking more/less at the mother? #TBD </p> <p>Do we want to give the player feedback that (s)he did look at the caregiver? #TBD </p> <ul> <li>we could illuminate the caregiver (\"standing in the spotlight\")</li> <li>we could have a HUD appear and have the conversation there.</li> </ul>"},{"location":"architecture/game_elements/caregiver/#score-the-caregiver","title":"Score the Caregiver","text":"<p>The player must indicate how much (s)he trusts the caregivers advice. </p> <p>A scoring is given on a scale of 0-10? #TBD</p>"},{"location":"architecture/game_elements/caregiver/#game-mechanic_1","title":"Game Mechanic","text":"<p>We visualise a circle being formed by spreading the hands of the child. The bigger the circle, the more (s)he trusts the caregiver. In the middle of the circle we see the number change, number as well as color.</p> <p>Think: you close your body when you don't trust a person, you open your arms when you trust this person more.</p> <p>Pulling one of the triggers confirms the choice. </p>"},{"location":"architecture/game_elements/caregiver/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/caregiver/#rating-bar","title":"Rating Bar","text":"<p>see caregiver#game mechanic</p>"},{"location":"architecture/game_elements/caregiver/#mother","title":"Mother","text":""},{"location":"architecture/game_elements/caregiver/#caregiver-body","title":"Caregiver Body","text":"<p>Female body, slightly to be adapted to the mothers figure?</p> <p>Needs to be rigged for animation.</p>"},{"location":"architecture/game_elements/caregiver/#tbd","title":"TBD","text":""},{"location":"architecture/game_elements/caregiver/#caregiver-face","title":"Caregiver Face","text":"<p>Nice to have: be able to adapt the face in- or pregame to resemble more or less the mother.</p>"},{"location":"architecture/game_elements/caregiver/#caregiver-comms-system","title":"Caregiver Comms System","text":""},{"location":"architecture/game_elements/caregiver/#hud-instead-of-in-game-conversation","title":"HUD Instead of In-game Conversation","text":"<p>perhaps interesting to have a 2D sprite showing the conversation?  </p> <p>Combination of the 3D world environment with a HUD (AR within VR) gives perhaps more emotional depth?</p> <p></p> <ul> <li>We definitely want a button to command them to push it before restarting the trial. </li> <li>We keep track of the time of reading till pushing #metric</li> <li>we also set the marker in data of appearance of the HUD/feedback + eye tracking if not via HUD.</li> </ul>"},{"location":"architecture/game_elements/caregiver/#styling-the-mother","title":"Styling the Mother","text":"<p>Can or should the participant be able to style his/her own mother?</p> <p>the look of the mother: in-game vs out of the game #TBD:</p> <p>%%- Can we use volumetric scanning to capture the mothers face realistically enough?     - Can we let the child firstly create his/her mothers face?             - this could be done outside of VR, lowering the bar to enter the VR environment YET This means we have to recompile each time, raising the bar for errors?%%</p> <ul> <li>in-game:         - a selection mechanism in which the child can choose hairstyles and such. Could work in a non-realistic style.         - Attention: this could take more time in-game?<ul> <li>Nice to have: tweaking a mother persona (colour of hair, eyes, length, ...)</li> </ul> </li> </ul>"},{"location":"architecture/game_elements/caregiver/#animation","title":"Animation","text":"<p>I think a lot of focus can be put in the animation of different emotions of the face. </p> <p>How can we create attention to a certain emotional view?</p> <ul> <li>only facial expressions?</li> <li>body language?<ul> <li>recordings via mocap?</li> </ul> </li> </ul>"},{"location":"architecture/game_elements/metrics/","title":"Metrics","text":"<p>in this project, we'll get to record differing parameters.</p> <p>In order to get an overview, we'll list them in this file as they get decided throughout the research project.</p> metric type how to calculate/record sensor in-game or data processing engagement ball flag if a child always shoots the ball immediately away, or is showing no interest at all in the ball VR in-game brain activity EEG EEG headset EEG in-game start and stop the recording response read time VR start of new trial VR eye tracking of response? EOG"},{"location":"architecture/game_elements/metrics/#metrics-transfer","title":"Metrics Transfer","text":"<p>the VR application will be running on the android device. Either we try to send all data wirelessly/via usb cable to the attached computer (Need to open port?)</p> <p>Or we try to save the data on the VR headset and later transfer it to the server.</p> <p>I prefer transmitting it via a websocket/MQTT.</p>"},{"location":"architecture/game_elements/metrics/#poc","title":"POC","text":""},{"location":"architecture/game_elements/metrics/#metrics-folder","title":"Metrics Folder","text":"<p>structure:</p> <ul> <li>per child we have a folder</li> <li>in the folder we have:<ul> <li>two folders for each block</li> <li>in each block:<ul> <li>a metric file for the EOG data</li> <li>a metric file for the EEG data</li> <li>a metric file for the GSR data</li> <li>a response file for the caregiver scorings</li> <li>a setup file for each trial</li> <li>a file from KUL with their questionnaires.</li> </ul> </li> </ul> </li> </ul> <p>The unity application should be the master, the EEG (python script with LSLMarker stream) and GSR (openframeworks+ LSLMarker stream) are masons.</p>"},{"location":"architecture/game_elements/metrics/#decisions","title":"Decisions","text":""},{"location":"architecture/game_elements/metrics/#eog","title":"EOG","text":"<p>We put some objects on the Unity_layers#eye. These metrics we store per trial in the metrics file.</p>"},{"location":"architecture/game_elements/metrics/#metrics-of-eog","title":"Metrics of EOG","text":"<p>Per trial:</p> <ul> <li>per contact:<ul> <li>when did the child look at the caregiver?</li> <li>for how long?</li> </ul> </li> <li>given a minimum time between two consecutive 'looks', how often did the child look at the caregiver?</li> </ul> <p>Per block of trials:</p> <ul> <li>do we need some metric? These can be deducted I believe out of the trials</li> </ul>"},{"location":"architecture/game_elements/reward_system/","title":"Reward System","text":"<p>If a player shoots in one of the correct targets, then (s)he sees an immediate reward.</p> <p>Above the target a particle appears, such as a star appearing above the goal.</p> <p></p> <p>These rewards are counted and passed on to the score_board.</p>"},{"location":"architecture/game_elements/reward_system/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/reward_system/#rewards-prefab","title":"Rewards Prefab","text":"<p>stars to be instantiated when scoring.</p> <p>Should hover above the goal for x time and then destroy itselves.</p>"},{"location":"architecture/game_elements/score_board/","title":"Score Board","text":"<p>We're constantly showing a score board in the VR environment.</p> <p></p> <p>After a trial round (5 consecutive throws for NPC &amp; player) we update the screen. First a particle is launched drawing the attention to the board and we see our own position and that of other (visible and non-visible) NPCs switch places in the ranking.</p>"},{"location":"architecture/game_elements/score_board/#assets","title":"Assets","text":"<p>1x score board, non-interactable</p>"},{"location":"architecture/game_elements/score_board/#scripts","title":"Scripts","text":"<ol> <li>update scoreboard script<ol> <li>launches a particle firstly drawing the attention</li> <li>name positions swap place when needed</li> <li>a script capturing the scores of the visible players + generating random scores for other players on the board.</li> </ol> </li> </ol>"},{"location":"architecture/game_elements/shooting_system/","title":"Shooting System","text":"<p>In shooting system we take a closer look at the shooting mechanism.  We want to child to be seated, yet have the feeling to be participating in a real competition We want to be able to influence the performance of the child when needed</p> <ul> <li>Necessary to be seated and not to move too much\u00a0<ul> <li>EEG readings otherwise compromised</li> </ul> </li> </ul>"},{"location":"architecture/game_elements/shooting_system/#overview","title":"Overview","text":"<p>I have one main concepts in mind: </p> <ul> <li>the slingshot</li> </ul>"},{"location":"architecture/game_elements/shooting_system/#slingshot","title":"Slingshot","text":"<p>The player is seated in some sort of chair, in front of you there's a large slingshot.</p> <p>When it's your turn to shoot, a ball appears next to you. You grab it and attach it to the slingshot. </p> <p>Once this is done, you can pull the slingshot and shoot it. The harder you pull, the harder the ball flies. ? Bart Simpson Slingshot ? </p> <p>We make use of Unity's spring joints to shoot the balls away for the real player, yet we also must be able to correct the ball towards the right goal? #TBD_Thomas_waterzooi (Slerp? gravity? pulling force from correct target?)</p> <p>Also the NPC we should steer the ball into the correct or incorrect direction. (Slerp)</p>"},{"location":"architecture/game_elements/shooting_system/#targets","title":"targets","text":"<p>On the other side of the room, there are either pipes or trash nets which serve as targets for the balls. #TBD</p> <p> </p>"},{"location":"architecture/game_elements/shooting_system/#shooting","title":"Shooting","text":"<ul> <li>assistance: difficulty level? <p>0026cae5192f3e64703937b6dedad855577db1ea:docs/architecture/game_elements/shooting_system.md</p> <ul> <li>how will the NPC be assisted<ul> <li>if the assistance is allowed for the real player in config file, then we make sure the NPC scores less/badly</li> </ul> </li> <li>how can the player be assisted?<ul> <li>If the ball is in reach of goal AND assistance is allowed (in config file), then we aid the trajectory of the ball.</li> </ul> </li> </ul> </li> <li>Have an assisting lineray showing were the ball would approx fly?</li> </ul>"},{"location":"architecture/game_elements/shooting_system/#scoring","title":"Scoring","text":"<p>Having the ball in the correct target gives 1 point.</p> <p>Depending on how fast you can score, you get an extra point?</p>"},{"location":"architecture/game_elements/shooting_system/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/shooting_system/#slingshot_1","title":"Slingshot","text":"<p>we need a slingshot with which we can interact</p>"},{"location":"architecture/game_elements/shooting_system/#balls","title":"Balls","text":"<p>colored balls, perhaps taking their color only slowly?</p> <p>Or changing color after time? </p> <p>! We want to be able to steer the performance of the child. I suggest we have some assistive trajectory change when we want to help the child.</p>"},{"location":"architecture/game_elements/targets/","title":"Targets","text":"<p>Multiple options of targets which can have a differing feel, can be decided along the way.</p>"},{"location":"architecture/game_elements/targets/#coloured-pipes","title":"Coloured Pipes","text":"<ul> <li>three coloured pipes, which either/combination<ul> <li>move a little bit, sometimes fast, sometimes slow ('snakes')</li> <li>have sporadically an inward airflow, so the ball can sometimes be sucked in the wrong vent</li> <li>the ball which we throw switches colour, make sure you send the ball in the right pipe. Yet the colour changes (bomb effect with glow \"he's almost going to change color!\")</li> </ul> </li> </ul>"},{"location":"architecture/game_elements/targets/#trash-cans","title":"Trash Cans","text":"<p>there are multiple trash cans positioned over the playfield. </p> <p>Above each trash can there's a spot aimed downwards, showing which trash can is the current target.</p> <p>There's a random countdown for when to switch spots and thus to decide which is the correct goal.</p>"},{"location":"architecture/game_elements/trial/","title":"Trial","text":"<p>A trial exists of a pre-trial phase, a trial phase and a post-trial phase.</p> <p>Per trial some metrics are defined:</p> <ol> <li>number of the trial: integer</li> <li>Assistance level for the child: bool<ol> <li>This implicates:<ol> <li>If True: the child gets aided to win</li> <li>If False: the child is not allowed to win</li> </ol> </li> </ol> </li> <li>Response for the caregiver: string (+ perhaps an audio sample)</li> </ol>"},{"location":"architecture/game_elements/trial/#pre-trial","title":"Pre Trial","text":"<p>Nothing to be done here, I guess.</p>"},{"location":"architecture/game_elements/trial/#trial_1","title":"Trial","text":"<p>The player and the NPC shoot balls alternating: one at the time, up to 5 balls. </p> <p>This should be fast paced action.</p> <p>shooting_system</p> <p>If the trial is with assistance, we help either the child, or we make the NPC to not score (or both).</p>"},{"location":"architecture/game_elements/trial/#post-trial","title":"Post Trial","text":"<p>The caregiver gives advice to the child. </p>"},{"location":"architecture/game_elements/trial/#tbd-correct-that-the-child-does-not-have-to-watch-the-caregiver-in-order-to-get-the-response","title":"TBD correct that the child does not have to watch the caregiver in order to get the response?","text":"<p>The child gets a menu to score the caregivers response. caregiver#score the caregiver</p> <p>We record whether the child did look at the caregiver metrics</p>"},{"location":"architecture/game_elements/trial/#count-down-timer","title":"Count-down Timer","text":"<p>after giving the score, a countdown is shown on the screen, indicating that the new trial is getting started. We move up one trial in the trialblock and set the phase to pretrial again.</p>"},{"location":"architecture/game_elements/trial/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/trial/#trialmanager","title":"Trialmanager","text":"<p>a state machine switching the trial phases and updating the trialnumber once a trial has finished.</p>"},{"location":"architecture/game_elements/trial/#countdown-timer","title":"Countdown Timer","text":"<p>Large numbers counting down on the screen, with faded lights/foggy view?</p> <p></p>"},{"location":"architecture/game_elements/trialblock/","title":"Trialblock","text":"<p>A trialblock consists of x trials and optionally a [[test-trial]]</p> <p>The block starts with:</p> <ul> <li>resetting the scores+score_board</li> <li>deciding the name of the trialblock:<ul> <li>block___name <li>this name is used to set up the Data streams</li> <li>if the last trial is running, show a \"congrats\" message when finishing instead restarting the trial, also have the score_board more in view</li>"},{"location":"architecture/game_elements/trialblock/#start-of-a-block","title":"Start of a Block","text":"<p>Perhaps to start a block, we have a short \"arrival\" moment, in which the child 'awakens' in the room. Before the lights go on, we already have some sounds. This is a transistion between the real, physical world and the new gameworld.</p>"},{"location":"architecture/game_elements/trialblock/#assets","title":"Assets","text":""},{"location":"architecture/game_elements/trialblock/#gameplay-manager","title":"Gameplay Manager","text":"<p>have a gameplay manager prefab, which has a fixed list of trials, fixed order and the contingency level per trial</p>"},{"location":"architecture/game_elements/trialblock/#wake-up-controller","title":"Wake-up Controller","text":"<p>This controller is launched as an event at the start of a block. Some sounds are played to get the mood on, the lights are down and are gradually launched. The player is welcomed with a screen, showing his opponent for the game and to wish him/her good luck.</p>"},{"location":"architecture/game_elements/trialblock/#welcome-screen","title":"Welcome Screen","text":"<ul> <li>Shows an image/gif of the other contestant</li> <li>wishes the player good luck</li> <li>has a button to start the game: <code>start</code></li> </ul>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/","title":"Overview of All Prefabs","text":""},{"location":"architecture/game_elements/DEPRECATED/prefabs/#imports","title":"Imports","text":"<p>Make sure you import the Tobii.XR and PicoXR SDK packages</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#list","title":"List","text":""},{"location":"architecture/game_elements/DEPRECATED/prefabs/#gamesetupprefab","title":"gameSetupPrefab","text":"<p>a prefab which is partly directed by the child, partly by the researchers. (do we want this to be decided upfront?)</p> <p>Also the caregiver can be shaped via this prefab? (if the child gets to form the caregiver)</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#statemanagerprefab","title":"stateManagerPrefab","text":"<ol> <li>keeps track where we are in a trial:</li> <li><code>preTrial</code>:</li> <li><code>Trial</code></li> <li><code>endTrial</code> </li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#triallistprefab","title":"trialListPrefab:","text":"<ol> <li>a list of all upcoming trials<ol> <li>each trial is calculated according to <code>public int contingencyLevel</code> to have a distribution of good and bad trials.</li> <li>the <code>trialListPrefab</code> dictates the caregiver out of which lists she should be giving a response if activated.</li> <li>the <code>trialListPrefab</code> decides if the upcoming trial will be <ol> <li>easy/difficult for competition (and level of NPCs)</li> <li>exclusion/normal game for ostracity</li> </ol> </li> </ol> </li> <li>keeps track of current trial via <code>private int currentTrial</code></li> <li>if a <code>nextTrial</code> signal is given, <code>currentTrial</code> goes up if not last trial<ol> <li>we store the important values in 'playerprefs': playerprefs or via a singleton: singleton</li> </ol> </li> <li>otherwise we show an endImage</li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#caregiverprefab","title":"caregiverPrefab:","text":"<ol> <li>a model with animation rigging</li> <li>public bool <code>canBeActivated</code>: makes it possible to activate the caregiver</li> <li>if <code>practicalSetup</code> &amp; <code>preTrial</code>: caregivers <code>canBeActivated</code> is set to True until <code>startTrial</code> is activated OR <code>hasBeenActivated</code> is set to true</li> <li>if <code>emotionalSetup</code> &amp; <code>postTrial</code>: caregivers <code>canBeActivated</code> is set to True until <code>nextTrial</code> is activated OR <code>hasBeenActivated</code> is set to true</li> <li>public bool <code>isActivated</code>: </li> <li>when activated the caregiver reads a random line of one of both responseLists according to the <code>contingencyMode</code></li> <li>also private bool <code>hasBeenActivated</code> gets set to true UNTIL <code>nextTrial</code> signal is given.</li> <li>two versions: practical &amp; emotional:</li> <li>four differing lists of possible responses.</li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#responselistprefab","title":"responseListPrefab","text":"<ol> <li>we need four lists:</li> <li>emotional responses:<ol> <li>supportive</li> <li>non-supportive</li> </ol> </li> <li>practical responses:<ol> <li>good advice</li> <li>bad advice</li> </ol> </li> <li>these lists can be csv format, so they can be maintained outside of unity</li> <li>we have a function returnResponse(supportType, goodOrBadResponse) which returns a random string from the corresponding list.</li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#scoreboardprefab","title":"scoreBoardPrefab","text":"<ol> <li>only necessary in competition</li> <li>has three variables: one for each player</li> <li>if <code>endTrial</code> signal is given the score is updated for each player </li> <li>An animation is shown indicating the ranking of the child.</li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#caregiverscoringprefab","title":"caregiverScoringPrefab","text":"<ol> <li>a 2D image being presented in which the child must indicate on a line how much (s)he trusts the caregiver.</li> <li>the score is saved via the saveMetricsPrefab</li> <li></li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#savemetricsprefab","title":"saveMetricsPrefab","text":"<p>a prefab which has one/multiple files open to write to them</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#text-balloon","title":"Text Balloon","text":"<ol> <li>a 3D balloon, oriented towards the player</li> <li>should show text very clearly</li> <li>show some particles to draw attention?</li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#ball","title":"Ball","text":"<p>the ball has a state machine:</p> <ol> <li>being grabbed<ol> <li>the ball is interactable by a player</li> <li>the ball changes to this state when in a certain radius of the player and a raycast hits </li> </ol> </li> <li>being attached<ol> <li>the ball is parented to a player's hand</li> <li>the ball changes to this state when collided with the players hand</li> <li>the player does not look anymore to the ball itself. (if the ball's parent is itself or a child of itselves, don't look)</li> </ol> </li> <li>being thrown<ol> <li>by an NPC: via slerp<ol> <li>after x time, at y speed</li> </ol> </li> <li>by the player: via XR rig<ol> <li>with assistance if needed (boolean or floating range of support)</li> <li></li> </ol> </li> </ol> </li> <li>being idle<ol> <li>can be attracted if close enough?</li> <li>should be destroyed and respawn in a players hand after x time</li> <li>a ball gets in the idle state if the child did throw the ball and it's not close enough for the other players to attract the ball.</li> </ol> </li> </ol>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#login-screen-prefab","title":"Login Screen Prefab","text":"<p>When the child enters the game for the first time, we need to show a text block in which all steps are explained.</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#chooseteamprefab","title":"chooseTeamPrefab","text":"<p>child gets to see in-game menu to choose team from, this is a screen that is fixed in position, the child can look away from it towards the caregiver.</p> <p>if this is shown, no players should be on the field.</p> <p>Sound from Zapsplat.com</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#issues","title":"Issues","text":"<p>when parenting a ball, strange things happen. I think it's primarily because the ball was moving before and I've parented it with keeping the location. Yet this does probably NOT stop the movement due to the mass of the object? therefore the ball can be at large distances instead of attached to the hand (as intended)</p> <p>Project settings ==&gt; Player ==&gt; Scripting Define Symbols ==&gt; DEFINE_NAME ==&gt; #if DEFINE_NAME #endif possible!</p> <p>set all environment things to a layer and lock it</p> <p>[System.Serializable] boven een class maakt dat je niet monobehavior component kan scripten</p> <p>colliders overview!</p> <p>preferences ==&gt; general ==&gt; Script changes while playing</p> <p>tooltip attributes</p> <p>Slerp</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#questions","title":"Questions","text":"<p>data bijhouden: 1 grote file of liever per topic aparte file, in 1 folder per kind?</p> <p>TODO: UI scherm aan de zijkant waarop je van scene kunt switchen?</p>"},{"location":"architecture/game_elements/DEPRECATED/prefabs/#todo-contingentie-aanpassen","title":"TODO Contingentie Aanpassen","text":"<p>statemanager: nu emotional vs practical , maar ook al competition vs ostracity?</p> <p>contingentie: wat mama zegt en of het goed of fout is</p> <p>isGoodTrial mag random zijn</p> <p>maar isGoodResponse is gelinkt aan het advies hier op</p> <p>tekst: zwevend in ruimte bij zorgfiguurn + pijltje naar de mond</p> <p>licht laten uitgaan bij eindigen?</p> <p>sporthall: achtergrond extra spelers + geluiden</p> <p>supporters aan de zijkant?</p> <p>status: checken of alle prefabs goed staan in beide scenes, kunnen we switchen?</p>"},{"location":"hardware/EEG/","title":"OpenBCI EEG Headset","text":""},{"location":"hardware/EEG/#physical-adaption","title":"Physical Adaption","text":""},{"location":"hardware/EEG/#testing","title":"Testing","text":"<p>In the OpenBCI GUI we check if we get a signal from all electrodes. </p>"},{"location":"hardware/EEG/#brainflow-library","title":"Brainflow library","text":"<p>pylsl needs a specific library on the path, liblsl64.dll To solve this, remove it if it was installed via conda. Install via pip again.</p> <p>Inserting markers gives an error, why??</p>"},{"location":"hardware/EEG/#installation","title":"installation","text":"<p>testing via OpenBCI GUI</p>"},{"location":"hardware/GSR/","title":"Emotibit","text":""},{"location":"hardware/GSR/#installation","title":"Installation","text":"<p>https://github.com/EmotiBit/EmotiBit_Docs</p>"},{"location":"hardware/GSR/#approach","title":"Approach","text":"<p>sends data via WiFi (credentials on SD card) ==&gt; need for accessible network (no eduroam!)</p> <p>three approaches: we capture the data via the emotibit oscilloscope. </p> <ol> <li>in the oscilloscope push record, so the data is saved on the SD card ==&gt; highest resolution</li> <li>We can transmit the data over OSC towards a python capture script. This can be tied to markers given from Unity?</li> <li> <p>We create an LSL marker stream from unity to annotate a stream.</p> <p>update: from link it came to my attention that there's no timestamp present in the OSC stream, which I was planning to capture. Timestamping is insecure.  Conclusion It is best to use the SD card to save the data!</p> </li> </ol>"},{"location":"hardware/GSR/#example-data","title":"Example Data","text":"<p>example data can be found in folder emotibit</p>"},{"location":"hardware/GSR/#lsl-approach","title":"LSL Approach","text":"<p>to test you can generate an LSL stream via ofxLSL</p>"},{"location":"hardware/GSR/#starting-via-python","title":"starting via python?","text":"<p>check link to see if we can use serial commands?</p> <p>also adapt the oscilloscope parameters: https://github.com/EmotiBit/EmotiBit_Docs/blob/master/Working_with_emotibit_data.md/#EmotiBit-Oscilloscope </p> <p>LSL converted markers can be found, after parsing the data in the _LM** file, with their timings.</p>"},{"location":"hardware/GSR/#unity","title":"Unity","text":"<p>add package via package mgr ==&gt; add from git url: LSL4Unity</p> <p>Import their samples to see how to create a stream.</p> <pre><code>using System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing LSL;\n\nnamespace LSL4Unity.Samples.SimplePhysicsEvent\n{\n    public class SimpleOutletTriggerEvent : MonoBehaviour\n    {\n        /*\n         * This is a simple example of an LSL Outlet to stream out irregular events occurring in Unity.\n         * This uses only LSL.cs and is intentionally simple. For a more robust version, see another sample.\n         * \n         * We stream out the trigger event during OnTriggerEnter which is, in our opinion, the closest\n         * time to when the trigger actually occurs (i.e., independent of its rendering).\n         * A simple way to print the events is with pylsl: `python -m pylsl.examples.ReceiveStringMarkers`\n         *\n         * If you are instead trying to log a stimulus event then there are better options. Please see the \n         * LSL4Unity SimpleStimulusEvent Sample for such a design.\n         */\n        [SerializeField] private string StreamName = \"DataSyncMarker\";//LSL4Unity.Samples.SimpleCollisionEvent\";\n        [SerializeField] private string StreamType = \"Markers\";\n        [SerializeField] private string StreamId = \"12345\";\n\n        private StreamOutlet outlet;\n        private string[] sample = {\"\"};\n\n        void Start()\n        {\n            StreamInfo streamInfo = new StreamInfo(StreamName, StreamType, 1, LSL.LSL.IRREGULAR_RATE,\n                channel_format_t.cf_string, StreamId);// hash.ToString());\n            outlet = new StreamOutlet(streamInfo);\n        }\n\n        private void OnTriggerEnter(Collider other)\n        {\n            if (outlet != null)\n            {\n                sample[0] = \"TriggerEnter \" + gameObject.GetInstanceID();\n                outlet.push_sample(sample);\n            }\n        }\n\n        private void OnTriggerExit(Collider other)\n        {\n            if (outlet != null)\n            {\n                sample[0] = \"TriggerExit \" + gameObject.GetInstanceID();\n                outlet.push_sample(sample);\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"hardware/Overview/","title":"Overview of Hardware Components","text":"<p>We'll be using following hardware components:</p> <ol> <li>Pico Neo Eye 3 Pro VR headset</li> <li>OpenBCI 16channel cython-EEG headset</li> <li>Emotibit biometric sensor</li> </ol>"},{"location":"hardware/Overview/#vr","title":"VR","text":"<p>VR Glass</p>"},{"location":"hardware/Overview/#eeg","title":"EEG","text":"<p>EEG</p>"},{"location":"hardware/Overview/#gsr","title":"GSR","text":"<p>GSR</p>"},{"location":"hardware/Overview/#naive-walkthrough","title":"naive walkthrough","text":"<p>This is a non-optimised flow to check how to fix things necessary.</p>"},{"location":"hardware/VR%20Glass/","title":"Pico Neo Eye 3 Pro","text":"<p>product link</p>"},{"location":"hardware/VR%20Glass/#eye-tracking","title":"Eye Tracking","text":"<p>Tobii tracking is added to the device</p>"},{"location":"hardware/VR%20Glass/#connecting-to-second-screen","title":"Connecting to Second Screen","text":"<p>Via displayport cable</p>"},{"location":"hardware/VR%20Glass/#unity-development-for-picon-neo-eye-3-pro","title":"Unity Development for Picon Neo Eye 3 Pro","text":""},{"location":"hardware/VR%20Glass/#prerequisites","title":"Prerequisites","text":"<p>In unity you need to add next packages manually, via \"window -&gt; package manager -&gt; add from disk\":</p> <ul> <li>Pico Unity Integration SDK </li> <li>Tobii SDK</li> </ul> <p>Next adapt following settings:</p> <ol> <li>enable PicoXR</li> <li>have a high enough version of android</li> <li>use IL2CPP instead of mono</li> <li>build for ARM64</li> <li>Make sure input system is \"Both\" (old and new)</li> <li>install package \"unity inputsystem\"</li> <li>install package XR Interaction toolkit: link<ol> <li>Import the Starter Assets!</li> </ol> </li> <li>In \"player settings\", make sure underneath \"XR plugin mgmt\" PicoXR is enabled</li> </ol> <p>Tip: visit this link to get started</p>"},{"location":"hardware/VR%20Glass/#set-up-vr-space","title":"Set up VR Space","text":"<p>Check this link</p>"},{"location":"hardware/VR%20Glass/#vr-grabbable","title":"VR Grabbable","text":"<p>Releasing an object</p> <p>Use the \"Selected Exited\" event to release an object. (create a function and attach it)</p> <p>showing trajectory</p>"}]}